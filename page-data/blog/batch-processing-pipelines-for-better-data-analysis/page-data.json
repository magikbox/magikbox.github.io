{"componentChunkName":"component---src-templates-post-js","path":"/blog/batch-processing-pipelines-for-better-data-analysis/","result":{"data":{"ghostPost":{"id":"Ghost__Post__5ec2ca287aa22c4066f83b68","title":"Batch Processing Pipelines for Better Data Analysis","slug":"batch-processing-pipelines-for-better-data-analysis","featured":false,"feature_image":"https://gojek-ghost.zysk.in/content/images/2020/05/1_EbT2AH9uAMjxHrVxh5xJrw.jpeg","excerpt":"How we generate intelligible insights from our data warehouse using batch pipelines.","custom_excerpt":"How we generate intelligible insights from our data warehouse using batch pipelines.","visibility":"public","created_at_pretty":"18 May, 2020","published_at_pretty":"11 November, 2019","updated_at_pretty":"18 May, 2020","created_at":"2020-05-18T23:17:20.000+05:30","published_at":"2019-11-11T09:30:00.000+05:30","updated_at":"2020-05-18T23:34:54.000+05:30","meta_title":null,"meta_description":"How we generate intelligible insights from our data warehouse using batch pipelines.","og_description":null,"og_image":null,"og_title":null,"twitter_description":null,"twitter_image":null,"twitter_title":null,"authors":[{"name":"Gojek","slug":"gojek","bio":null,"profile_image":null,"twitter":"@gojektech","facebook":null,"website":"http://www.gojek.io"}],"primary_author":{"name":"Gojek","slug":"gojek","bio":null,"profile_image":null,"twitter":"@gojektech","facebook":null,"website":"http://www.gojek.io"},"primary_tag":{"name":"Data","slug":"data","description":"Updates on Gojek's work in Data Science and Data Engineeringâ€”from infrastructure development to our experiments with AI and ML.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"},"tags":[{"name":"Data","slug":"data","description":"Updates on Gojek's work in Data Science and Data Engineeringâ€”from infrastructure development to our experiments with AI and ML.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"}],"plaintext":"By Maulik Soneji\n\nGojekâ€™s Data Warehouse [https://en.wikipedia.org/wiki/Data_warehouse], built by\nintegrating data from multiple applications and sources, serves as a central\npoint of analysis that also helps generate actionable insights. Our batch\npipelines process billions of data points periodically, in order to help our\nbusiness teams gather an effective view of data.\n\nThis post explains our approach to building batch pipelines that leverage\ncomplex data in an efficient way.\n\nI will start by providing some context on our data warehouse and the data we\nstore in it, and explain the use cases of batch processing that we tackle at\nGojek. Then weâ€™ll talk about how we tackle running batch processing jobs and\nhandling scheduling dependencies.\n\nData Warehouse setup\nAt Gojek, we use Google Bigquery (BQ) as a Data Warehouse. All data points\nranging from booking and searches to location are published in real time using \nBeast [https://github.com/gojek/beast], our open-sourced tool.\n\nWe also push data from Kafka to our data lake, which is Google Cloud\nStorage(GCS). These data points vary in terms of the data format (which might\nnot be relevant to the analysts). They need to gather insights from the data\nwithout needing to know how it is stored.\n\n> To solve this, we wanted to create an abstraction such that the users of data\nonly need to know about the constitution of data, and not about where the data\nis coming from or what format the data is stored.\nBatch processing use cases\nTypical use cases of batch processing at Gojek revolve around enriching\nreal-time data with additional data points mined from huge amounts of historical\ndata.\n\nA few examples of use cases include:\n\nCreating a customer profile:\n\nIn order to provide our customers with the most relevant discount and deal\nvouchers, we enrich customer profiles with the last few months of the customerâ€™s\norder and search history. This enables our team of data analysts and data\nscientists to experiment with customer segmentation and targeting. This use case\nhas been covered in much detail by my colleague Mayank in this blog\n[https://blog.gojekengineering.com/how-we-solved-user-selection-to-help-merchants-win-business-519fe5085a0e]\n.\n\nPersonalising search results:\n\nIn order to personalise the search results served up by our food delivery app\nGoFood, we leverage batch processing to gather insights about trending, popular,\nand highly-rated restaurants near the user that match their taste profile. More\ndetails around how we went about this use case are covered in this blog\n[https://blog.gojekengineering.com/how-the-gojek-butler-serves-a-gourmet-meal-to-our-users-4a161d83052a]\n.\n\nRunning Batch pipelines\nAs I previously mentioned, the users of data usually donâ€™t need to know the\nformat in which the data is stored. They would benefit from having a unified\ninterface to interact with data.\n\nWe leveraged Dataframes in Apache Spark [https://spark.apache.org/] to provide\nthe unified interface.\n\nDataFrames or Datasets\nSpark provides an abstraction on top of the data underneath â€” called DataFrames\nor Datasets.\n\nDataFrames are distributed collections of data in which data is organised in the\nform of columns.\n\nConceptually, a data frame becomes similar to a database.\n\nFew examples of reading from Bigquery and GCS are as follows:\n\nClient to read data from bigquery into spark dataframe.\n\nClient to read data from GCS into spark dataframe\n\nUsing these clients make it very easy for our analysts to read GCS and Bigquery\ndata into Spark and interact with it.\n\nRunning Spark Jobs\nWe use Google Dataproc [https://cloud.google.com/dataproc/] hosting a Spark\ncluster to run our batch pipelines. On each trigger of a batch job, we create an\nephemeral cluster to run the job, which means that the cluster is destroyed\nafter the batch job completes.\n\nThe batch job is written in Pyspark\n[https://spark.apache.org/docs/2.2.0/api/python/pyspark.html], which all our\nanalysts are familiar with. This provides a good interface to interact with\nSpark Dataframes.\n\nScheduling Dependencies between Jobs\nAs the Spark jobs become more complex and handle many responsibilities, it\nbecomes important to break them down into simpler jobs that can be better\nmanaged.\n\n> But this breakdown brings more challenges.\nWe now have to make sure the related jobs are scheduled taking in mind the\nscheduling dependencies between different jobs.\n\nFor example:\nIf there are two jobs, the first one calculates the last 6 months of order\nhistory and the second job uses the order history to calculate the preferred\nlocations from which the customer has ordered, it becomes important to run the\nfirst job and then schedule the second job.\n\nOur solution to handle such scheduling dependencies is to use Apache Airflow\n[https://airflow.apache.org/]. This is a tool to programmatically schedule and\nmanage scheduling dependencies between different jobs.\n\nThe scheduling dependencies are written as a Directed Acyclic Graph (DAG) and we\nset a schedule for the DAG to run. Simple. ðŸ™‚\n\nWith Airflow, we are also able to assign retries for each job. In the instance\nof a job failing, Airflow will rerun the job by itself.\n\nAs a final precaution, we have also added Slack integration and StatsD metrics\nwith Airflow, in order to get alerts for when the jobs have failed and need to\nbe fixed.\n\nSo thatâ€™s all for this post. Hope you liked it! If youâ€™d like to work on cool\nproblems and help us scale a #SuperApp for Southeast Asia, make sure to check\nout gojek.jobs [http://bit.ly/2CvjmXv]. Until next time. ðŸ––\n\n\n--------------------------------------------------------------------------------\n\nWant our updates delivered straight to your inbox? Sign up for our newsletter!\n[https://mailchi.mp/go-jek/gojek-tech-newsletter]","html":"<p>By Maulik Soneji</p><p>Gojekâ€™s <a href=\"https://en.wikipedia.org/wiki/Data_warehouse\" rel=\"noopener\">Data Warehouse</a>, built by integrating data from multiple applications and sources, serves as a central point of analysis that also helps generate actionable insights. Our batch pipelines process billions of data points periodically, in order to help our business teams gather an effective view of data.</p><p>This post explains our approach to building batch pipelines that leverage complex data in an efficient way.</p><p>I will start by providing some context on our data warehouse and the data we store in it, and explain the use cases of batch processing that we tackle at Gojek. Then weâ€™ll talk about how we tackle running batch processing jobs and handling scheduling dependencies.</p><h1 id=\"data-warehouse-setup\">Data Warehouse setup</h1><p>At Gojek, we use Google Bigquery (BQ) as a Data Warehouse. All data points ranging from booking and searches to location are published in real time using <a href=\"https://github.com/gojek/beast\" rel=\"noopener\"><strong><strong>Beast</strong></strong></a><strong><strong>, </strong></strong>our open-sourced tool.</p><p>We also push data from Kafka to our data lake, which is Google Cloud Storage(GCS). These data points vary in terms of the data format (which might not be relevant to the analysts). They need to gather insights from the data without needing to know how it is stored.</p><blockquote><em><em>To solve this, we wanted to create an abstraction such that the users of data only need to know about the constitution of data, and not about where the data is coming from or what format the data is stored.</em></em></blockquote><h1 id=\"batch-processing-use-cases\"><strong>Batch processing use cases</strong></h1><p>Typical use cases of batch processing at Gojek revolve around enriching real-time data with additional data points mined from huge amounts of historical data.</p><p>A few examples of use cases include:</p><p><strong><strong>Creating a customer profile:</strong></strong></p><p>In order to provide our customers with the most relevant discount and deal vouchers, we enrich customer profiles with the last few months of the customerâ€™s order and search history. This enables our team of data analysts and data scientists to experiment with customer segmentation and targeting. This use case has been covered in much detail by my colleague Mayank in this <a href=\"https://blog.gojekengineering.com/how-we-solved-user-selection-to-help-merchants-win-business-519fe5085a0e\" rel=\"noopener\">blog</a>.</p><p><strong><strong>Personalising search results:</strong></strong></p><p>In order to personalise the search results served up by our food delivery app GoFood, we leverage batch processing to gather insights about trending, popular, and highly-rated restaurants near the user that match their taste profile. More details around how we went about this use case are covered in this <a href=\"https://blog.gojekengineering.com/how-the-gojek-butler-serves-a-gourmet-meal-to-our-users-4a161d83052a\" rel=\"noopener\">blog</a>.</p><h1 id=\"running-batch-pipelines\">Running Batch pipelines</h1><p>As I previously mentioned, the users of data usually donâ€™t need to know the format in which the data is stored. They would benefit from having a unified interface to interact with data.</p><p><em><em>We leveraged Dataframes in </em></em><a href=\"https://spark.apache.org/\" rel=\"noopener\"><em><em>Apache Spark</em></em></a><em><em> to provide the unified interface.</em></em></p><h2 id=\"dataframes-or-datasets\"><strong>DataFrames or Datasets</strong></h2><p>Spark provides an abstraction on top of the data underneath â€” called DataFrames or Datasets.</p><p>DataFrames are distributed collections of data in which data is organised in the form of columns.</p><p><strong><strong>Conceptually, a data frame becomes similar to a database.</strong></strong></p><p>Few examples of reading from Bigquery and GCS are as follows:</p><!--kg-card-begin: html--><script src=\"https://gist.github.com/mauliksoneji/48d7d84976ee5957de90e03ba2314540.js\"></script><!--kg-card-end: html--><p><em>Client to read data from bigquery into spark dataframe.</em></p><!--kg-card-begin: html--><script src=\"https://gist.github.com/mauliksoneji/0a8c12d3c7ecbe2c4794dbd039e03815.js\"></script><!--kg-card-end: html--><p><em>Client to read data from GCS into spark dataframe</em></p><p>Using these clients make it very easy for our analysts to read GCS and Bigquery data into Spark and interact with it.</p><p><strong><strong>Running Spark Jobs</strong></strong><br>We use <a href=\"https://cloud.google.com/dataproc/\" rel=\"noopener\">Google Dataproc</a> hosting a Spark cluster to run our batch pipelines. On each trigger of a batch job, we create an ephemeral cluster to run the job, which means that the cluster is destroyed after the batch job completes.</p><p>The batch job is written in <a href=\"https://spark.apache.org/docs/2.2.0/api/python/pyspark.html\" rel=\"noopener\">Pyspark</a>, which all our analysts are familiar with. This provides a good interface to interact with Spark Dataframes.</p><h1 id=\"scheduling-dependencies-between-jobs\"><strong>Scheduling Dependencies between Jobs</strong></h1><p>As the Spark jobs become more complex and handle many responsibilities, it becomes important to break them down into simpler jobs that can be better managed.</p><blockquote>But this breakdown brings more challenges.</blockquote><p>We now have to make sure the related jobs are scheduled taking in mind the scheduling dependencies between different jobs.</p><p><strong><strong>For example:</strong></strong><br>If there are two jobs, the first one calculates the last 6 months of order history and the second job uses the order history to calculate the preferred locations from which the customer has ordered, it becomes important to run the first job and then schedule the second job.</p><p>Our solution to handle such scheduling dependencies is to use <a href=\"https://airflow.apache.org/\" rel=\"noopener\">Apache Airflow</a>. This is a tool to programmatically schedule and manage scheduling dependencies between different jobs.</p><p>The scheduling dependencies are written as a Directed Acyclic Graph (DAG) and we set a schedule for the DAG to run. Simple. ðŸ™‚</p><p>With Airflow, we are also able to assign retries for each job. In the instance of a job failing, Airflow will rerun the job by itself.</p><p>As a final precaution, we have also added Slack integration and StatsD metrics with Airflow, in order to get alerts for when the jobs have failed and need to be fixed.</p><p>So thatâ€™s all for this post. Hope you liked it! If youâ€™d like to work on cool problems and help us scale a #SuperApp for Southeast Asia, make sure to check out <a href=\"http://bit.ly/2CvjmXv\" rel=\"noopener\">gojek.jobs</a>. Until next time. ðŸ––</p><hr><p>Want our updates delivered straight to your inbox? <a href=\"https://mailchi.mp/go-jek/gojek-tech-newsletter\">Sign up for our newsletter!</a></p>","url":"https://gojek-ghost.zysk.in/batch-processing-pipelines-for-better-data-analysis/","canonical_url":null,"uuid":"0d510c4e-75d7-409e-97c0-5414539cb91f","page":null,"codeinjection_foot":null,"codeinjection_head":null,"codeinjection_styles":null,"comment_id":"5ec2ca287aa22c4066f83b68","reading_time":3}},"pageContext":{"slug":"batch-processing-pipelines-for-better-data-analysis"}}}