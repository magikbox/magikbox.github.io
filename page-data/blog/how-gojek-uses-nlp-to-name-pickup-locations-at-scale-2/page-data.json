{"componentChunkName":"component---src-templates-post-js","path":"/blog/how-gojek-uses-nlp-to-name-pickup-locations-at-scale-2/","result":{"data":{"ghostPost":{"id":"Ghost__Post__5eb0f8af5524cd001e7391f4","title":"How Gojek Uses NLP to Name Pickup Locations at Scale","slug":"how-gojek-uses-nlp-to-name-pickup-locations-at-scale-2","featured":false,"feature_image":"https://res-2.cloudinary.com/hcq4cvthp/image/upload/q_auto/v1/ghost-blog-images/1_dFalBje-vQCkEY8Zrq9P5g.jpg","excerpt":"Introducing CartoBERT, a Natural Language Processing (NLP) model developed by Gojek’s Cartography Data Science team.","custom_excerpt":"Introducing CartoBERT, a Natural Language Processing (NLP) model developed by Gojek’s Cartography Data Science team.","visibility":"public","created_at_pretty":"05 May, 2020","published_at_pretty":"01 May, 2020","updated_at_pretty":"12 May, 2020","created_at":"2020-05-05T10:55:03.000+05:30","published_at":"2020-05-01T09:30:00.000+05:30","updated_at":"2020-05-12T11:51:38.000+05:30","meta_title":"How Gojek Uses NLP to Name Pickup Points at Scale","meta_description":"Introducing CartoBERT, a Natural Language Processing (NLP) model developed by Gojek’s Cartography Data Science team.","og_description":null,"og_image":null,"og_title":null,"twitter_description":null,"twitter_image":null,"twitter_title":null,"authors":[{"name":"Gojek","slug":"gojek","bio":null,"profile_image":null,"twitter":"@gojektech","facebook":null,"website":"http://www.gojek.io"}],"primary_author":{"name":"Gojek","slug":"gojek","bio":null,"profile_image":null,"twitter":"@gojektech","facebook":null,"website":"http://www.gojek.io"},"primary_tag":{"name":"Data","slug":"data","description":"Updates on Gojek's work in Data Science and Data Engineering—from infrastructure development to our experiments with AI and ML.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"},"tags":[{"name":"Data","slug":"data","description":"Updates on Gojek's work in Data Science and Data Engineering—from infrastructure development to our experiments with AI and ML.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"}],"plaintext":"When our customers want to use our ride hailing products like GoRide and GoCar,\nthey are presented with convenient, clearly named pickup points nearby. Here’s\nan example:\n\nThis saves customers the hassle of calling the driver partner, explaining where\nthey are, what colour clothes they are wearing, and so on. Our pickup points are\ndesigned to make lives easier for both customers and driver partners.\n\nThis is possible because the pickup points shown on the app are popular pickup\nlocations around the area. What’s more, the pickup point names are displayed\nexactly how customers driver partners usually refer to them.\n\nBut how do we manage to name so many pickup points accurately, and at scale?\n\nWe use past booking locations and their associated chat logs to discover named\npickup points. As our previous research has explained, we first perform \nclustering\n[https://blog.gojekengineering.com/fantastic-drivers-and-how-to-find-them-a88239ef3b29] \non historical bookings to form potential pickup points, then we use a language\nmodel\n[https://blog.gojekengineering.com/how-i-met-my-gojek-driver-without-a-single-call-95041f4fdd03] \nto select the best name. Here, we explain how we improved upon the previous\nstatistical language model with a state-of-the-art NLP model, which makes the\nentire naming exercise fully scalable. This is the magic behind all the pickup\npoints seen on the Gojek app.\n\nHow can we learn better?\nAs explained in our previous post, our original statistical language model\nselects the best pickup point name from the most probable n-grams extracted from\nbookings text. However, such a statistical language model doesn’t ‘understand’\nthe meaning of the texts, it simply chooses phrases with high frequencies\nwithout knowing the semantics. Sometimes it throws street names, sometimes even\ncommon phrases with no information about location. We have to manually check\neverything to make sure it reflects the right POI, before it appears on the app.\n\nThis creates a challenge — especially if we want to quickly expand the\nfrictionless pickup experience to customers across in new geographies. Hence, we\ndecided to go a step further with a deep-learning NLP model that ‘understands’\nand ‘learns’ to differentiate what is a valid pickup point name.\n\nAt Gojek, we never stop thinking and always go a step further\n\nMeet CartoBERT ?\nOne of the most recent and impactful breakthroughs NLP was the publication of\nBERT[1] — a contextual language representation with transformer models — by\nGoogle in late 2018. It obtained state-of-the-art results on a wide array of NLP\ntasks. In the 2019, many NLP researches were influenced by BERT, including\nXLNet, RoBERTa, ERNIE etc.\n\nBERT Explained\nBERT, or Bidirectional Encoder Representations from Transformers, is composed of\nan embedding layer, followed by groups of transformer layers.\n\nEvery word (token) in the input sentence will first get encoded into its\nembedding representations in the embedding layer, and then go through\nbidirectional transformer encoder layers. Every encoder layer will perform the\nmulti-head attention computation on the token representation from the previous\nlayer to create a new intermediate representation, which is then output to the\nnext layer. The output from the final layer is the contextual representation of\nthe input token. A pooled sentence level representation combining all token\nrepresentations could be created if needed by specific downstream tasks.\n\nWith the final contextual representations at either token or sentence level, a\npre-trained BERT on large unlabelled text corpus, could be further extended to a\nwide variety of NLP tasks, such as text classification, question answering,\nNamed Entity Recognition (NER) etc.\n\nALBERT[2], published by Google in Sep 2019, improved on BERT with embedding\nparameter factorisation and cross layer parameter sharing to reduce the number\nof parameters (by 9 times for base model). It also uses sequence order\nprediction instead of next sentence prediction for the pre-train task. In the\npaper, ALBERT also outperforms BERT on standard NLP tasks/datasets (SQUAD, RACE\netc), with fewer parameters.\n\nPre-train CartoBERT to learn language representation from Gojek bookings text\nInspired by ALBERT’s lightweight model and performance, we developed CartoBERT,\nGojek’s very own pickup point name recognition model, based on ALBERT’s\narchitecture.\n\nAs illustrated below, the uncased CartoBERT is pre-trained on Gojek’s own masked\nbookings text corpus of about 200 million sentences. Booking text is first\npre-processed for data masking to mask all customer sensitive information,\nlanguage detection, text normalisation (including text cleaning, slang,\nabbreviation transformations, lowercase transformation and emoji removal). The\npre-processed text is used to build subword vocabularies which handles\nOut-Of-Vocabulary (OOV) tokens that could be decomposed to frequent subword\npatterns. CartoBERT tokenizer is then created with the subword vocabularies and\nfurther used to encode and tokenize the same preprocessed bookings text to form\npre-trained input files.\n\nSame as ALBERT, the model is pre-trained to ‘understand’ Gojek’s bookings text\nusing Masked Language Model — which predicts randomly masked tokens in input\nsentences — and Sentence Order Prediction tasks, which predicts the order of\ninput sentences pair.\n\nFine-tuning CartoBERT to extract pickup point names from Gojek bookings text\nWith the huge amount of bookings text we have at Gojek, now CartoBERT can better\n‘understand’ past bookings text. Theoretically, it ‘understands’ every word of a\nbooking text sentence.\n\nFor every token in the input sentence, CartoBERT will output a 768-dimension\nvector (we use the default hidden layer size of the ALBERT base model in\nCartoBERT, however this is configurable) from last transformer encoder layer,\nand we use that to represent CartoBERT’s ‘understanding’ of the token’s meaning\nin the sentence context for fine-tune step.\n\nAs illustrated in the diagram below, while fine-tuning CartoBERT for pickup\npoint name recognition, we replace the Masked Language Model and Sequence Order\nPrediction layers from CartoBERT in pre-train step with token classification\nlayer. The token classification layer learns to predict the probability of a\ntoken belonging to a pickup point name, with the final token representation\noutput from CartoBERT transformer layers, from labelled training data created\nwith bookings text sentences, and corresponding pickup point names. Here, we use\nweighted cross entropy loss to deal with class imbalance, as tokens tagged to\npickup point names are a minority.\n\n\n\nWith this, CartoBERT is fine-tuned to extract pickup point names from bookings\ntext sentences, if any.\n\nHow does the model perform?\nCartoBERT gives a lift of more than 25% in pickup point name accuracy to ~93%\naccuracy, which is measured as the percentage of valid pick up point names out\nof generated names. With this high accuracy, we have achieved full scalability\nof automatic generation for named pickup points to quickly cover multiple\ngeographies without heavy reliance on human inputs.\n\nWhat’s next?\nWe are not stopping here and are exploring using active learning to further\nimprove CartoBERT. With active learning, we only flag out uncertain predictions,\nwhich are measured as sentence level least token probability[3] for human\nlabelling. We then use human-curated data as feedback for model learning. In\nthis way, we can improve model learning efficiency with minimum labelling\neffort.\n\nWhat’s more, with the success of CartoBERT, we are considering pre-training and\nopen sourcing a general Indonesia Bahasa ALBERT model with Indonesia open corpus\nfrom wiki, news, Twitter etc. Currently, the options for open-sourced language\nmodel in Indonesia Bahasa are very limited, only pre-trained static word\nembeddings such as word2vec, fasttext etc are available. It would be beneficial\nto the community if we have a good state-of-the-art attention-based transformer\nmodel for the language. Stay tuned for more updates from the Cartography Data\nScience team. ?\n\nLeave a ? if you liked what you read. Ping me with suggestions and feedback.\n\nThanks to all the amazing people who contributed to this post: Tan Funan, Zane\nLim, Dang Le, Lijuan Chia, Bani Widyatmiko, Maureen Koha, Ringga Saputra, Nur\nIzzahudinr, Sandya Ardi, Yeni Primasari, Ardya Dipta.\n\n\n--------------------------------------------------------------------------------\n\nReferences\n\n[1] J. Devlin [https://arxiv.org/search/cs?searchtype=author&query=Devlin%2C+J], \nM. Chang [https://arxiv.org/search/cs?searchtype=author&query=Chang%2C+M], K.\nLee [https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+K], K. Toutanova\n[https://arxiv.org/search/cs?searchtype=author&query=Toutanova%2C+K]: BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding.\narXiv:1810.04805 [https://arxiv.org/abs/1810.04805] (2018)\n\n[2] Z. Lan [https://arxiv.org/search/cs?searchtype=author&query=Lan%2C+Z], M.\nChen [https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+M], S. Goodman\n[https://arxiv.org/search/cs?searchtype=author&query=Goodman%2C+S], K. Gimpel\n[https://arxiv.org/search/cs?searchtype=author&query=Gimpel%2C+K], P. Sharma\n[https://arxiv.org/search/cs?searchtype=author&query=Sharma%2C+P], R. Soricut\n[https://arxiv.org/search/cs?searchtype=author&query=Soricut%2C+R]: ALBERT: A\nLite BERT for Self-supervised Learning of Language Representations. \narXiv:1909.11942 [https://arxiv.org/abs/1909.11942] (2019)\n\n[3] M.Liu [https://arxiv.org/search/cs?searchtype=author&query=Liu%2C+M], Z. Tu\n[https://arxiv.org/search/cs?searchtype=author&query=Tu%2C+Z], Z. Wang\n[https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Z], X. Xu\n[https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+X]: LTP: A New Active\nLearning Strategy for Bert-CRF Based Named Entity Recognition. arXiv:2001.02524\n[https://arxiv.org/abs/2001.02524] (2020)\n\n\n--------------------------------------------------------------------------------\n\nLiked what you read? Sign up for our newsletter\n[https://mailchi.mp/go-jek/gojek-tech-newsletter] to have our latest stories\ndelivered straight to your inbox!","html":"<p>When our customers want to use our ride hailing products like GoRide and GoCar, they are presented with convenient, clearly named pickup points nearby. Here’s an example:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://miro.medium.com/max/180/1*msS7z4IN06LVM0XvClmmPQ.gif\" class=\"kg-image\"></figure><p>This saves customers the hassle of calling the driver partner, explaining where they are, what colour clothes they are wearing, and so on. Our pickup points are designed to make lives easier for both customers and driver partners.</p><p>This is possible because the pickup points shown on the app are popular pickup locations around the area. What’s more, the pickup point names are displayed exactly how customers driver partners usually refer to them.</p><p><strong><strong>But how do we manage to name so many pickup points accurately, and at scale?</strong></strong></p><p>We use past booking locations and their associated chat logs to discover named pickup points. As our previous research has explained, we first perform <a href=\"https://blog.gojekengineering.com/fantastic-drivers-and-how-to-find-them-a88239ef3b29\" rel=\"noopener\">clustering</a> on historical bookings to form potential pickup points, then we use a <a href=\"https://blog.gojekengineering.com/how-i-met-my-gojek-driver-without-a-single-call-95041f4fdd03\" rel=\"noopener\">language model</a> to select the best name. Here, we explain how we improved upon the previous statistical language model with a state-of-the-art NLP model, which makes the entire naming exercise fully scalable. This is the magic behind all the pickup points seen on the Gojek app.</p><h1 id=\"how-can-we-learn-better\">How can we learn better?</h1><p>As explained in our previous post, our original statistical language model selects the best pickup point name from the most probable n-grams extracted from bookings text. However, such a statistical language model doesn’t ‘understand’ the meaning of the texts, it simply chooses phrases with high frequencies without knowing the semantics. Sometimes it throws street names, sometimes even common phrases with no information about location. We have to manually check everything to make sure it reflects the right POI, before it appears on the app.</p><p>This creates a challenge — especially if we want to quickly expand the frictionless pickup experience to customers across in new geographies. Hence, we decided to go a step further with a deep-learning NLP model that ‘understands’ and ‘learns’ to differentiate what is a valid pickup point name.</p><p><em><em>At Gojek, we never stop thinking and always go a step further</em></em></p><h1 id=\"meet-cartobert-\">Meet CartoBERT ?</h1><p>One of the most recent and impactful breakthroughs NLP was the publication of BERT[1] — a contextual language representation with transformer models — by Google in late 2018. It obtained state-of-the-art results on a wide array of NLP tasks. In the 2019, many NLP researches were influenced by BERT, including XLNet, RoBERTa, ERNIE etc.</p><h2 id=\"bert-explained\">BERT Explained</h2><p>BERT, or Bidirectional Encoder Representations from Transformers, is composed of an embedding layer, followed by groups of transformer layers.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://miro.medium.com/max/1226/1*qHFXdPcW_3UkLEsRJy2FRg.png\" class=\"kg-image\"></figure><p>Every word (token) in the input sentence will first get encoded into its embedding representations in the embedding layer, and then go through bidirectional transformer encoder layers. Every encoder layer will perform the multi-head attention computation on the token representation from the previous layer to create a new intermediate representation, which is then output to the next layer. The output from the final layer is the contextual representation of the input token. A pooled sentence level representation combining all token representations could be created if needed by specific downstream tasks.</p><p>With the final contextual representations at either token or sentence level, a pre-trained BERT on large unlabelled text corpus, could be further extended to a wide variety of NLP tasks, such as text classification, question answering, Named Entity Recognition (NER) etc.</p><p>ALBERT[2], published by Google in Sep 2019, improved on BERT with embedding parameter factorisation and cross layer parameter sharing to reduce the number of parameters (by 9 times for base model). It also uses sequence order prediction instead of next sentence prediction for the pre-train task. In the paper, ALBERT also outperforms BERT on standard NLP tasks/datasets (SQUAD, RACE etc), with fewer parameters.</p><h2 id=\"pre-train-cartobert-to-learn-language-representation-from-gojek-bookings-text\">Pre-train CartoBERT to learn language representation from Gojek bookings text</h2><p>Inspired by ALBERT’s lightweight model and performance, we developed CartoBERT, Gojek’s very own pickup point name recognition model, based on ALBERT’s architecture.</p><p>As illustrated below, the uncased CartoBERT is pre-trained on Gojek’s own masked bookings text corpus of about 200 million sentences. Booking text is first pre-processed for data masking to mask all customer sensitive information, language detection, text normalisation (including text cleaning, slang, abbreviation transformations, lowercase transformation and emoji removal). The pre-processed text is used to build subword vocabularies which handles Out-Of-Vocabulary (OOV) tokens that could be decomposed to frequent subword patterns. CartoBERT tokenizer is then created with the subword vocabularies and further used to encode and tokenize the same preprocessed bookings text to form pre-trained input files.</p><p>Same as ALBERT, the model is pre-trained to ‘understand’ Gojek’s bookings text using Masked Language Model — which predicts randomly masked tokens in input sentences — and Sentence Order Prediction tasks, which predicts the order of input sentences pair.</p><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"https://miro.medium.com/max/1386/1*STaqWssxlYPhFaDulLwJzg.png\" class=\"kg-image\"></figure><h2 id=\"fine-tuning-cartobert-to-extract-pickup-point-names-from-gojek-bookings-text\">Fine-tuning CartoBERT to extract pickup point names from Gojek bookings text</h2><p>With the huge amount of bookings text we have at Gojek, now CartoBERT can better ‘understand’ past bookings text. Theoretically, it ‘understands’ every word of a booking text sentence.</p><p>For every token in the input sentence, CartoBERT will output a 768-dimension vector (we use the default hidden layer size of the ALBERT base model in CartoBERT, however this is configurable) from last transformer encoder layer, and we use that to represent CartoBERT’s ‘understanding’ of the token’s meaning in the sentence context for fine-tune step.</p><p>As illustrated in the diagram below, while fine-tuning CartoBERT for pickup point name recognition, we replace the Masked Language Model and Sequence Order Prediction layers from CartoBERT in pre-train step with token classification layer. The token classification layer learns to predict the probability of a token belonging to a pickup point name, with the final token representation output from CartoBERT transformer layers, from labelled training data created with bookings text sentences, and corresponding pickup point names. Here, we use weighted cross entropy loss to deal with class imbalance, as tokens tagged to pickup point names are a minority.</p><p></p><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"https://miro.medium.com/max/1526/1*WO1LRHQPqAqzpLBMMl-NuA.png\" class=\"kg-image\"></figure><p>With this, CartoBERT is fine-tuned to extract pickup point names from bookings text sentences, if any.</p><h2 id=\"how-does-the-model-perform\">How does the model perform?</h2><p>CartoBERT gives a lift of more than 25% in pickup point name accuracy to ~93% accuracy, which is measured as the percentage of valid pick up point names out of generated names. With this high accuracy, we have achieved full scalability of automatic generation for named pickup points to quickly cover multiple geographies without heavy reliance on human inputs.</p><h1 id=\"what-s-next\">What’s next?</h1><p>We are not stopping here and are exploring using active learning to further improve CartoBERT. With active learning, we only flag out uncertain predictions, which are measured as sentence level least token probability[3] for human labelling. We then use human-curated data as feedback for model learning. In this way, we can improve model learning efficiency with minimum labelling effort.</p><p>What’s more, with the success of CartoBERT, we are considering pre-training and open sourcing a general Indonesia Bahasa ALBERT model with Indonesia open corpus from wiki, news, Twitter etc. Currently, the options for open-sourced language model in Indonesia Bahasa are very limited, only pre-trained static word embeddings such as word2vec, fasttext etc are available. It would be beneficial to the community if we have a good state-of-the-art attention-based transformer model for the language. Stay tuned for more updates from the Cartography Data Science team. ?</p><p>Leave a ? if you liked what you read. Ping me with suggestions and feedback.</p><p>Thanks to all the amazing people who contributed to this post: Tan Funan, Zane Lim, Dang Le, Lijuan Chia, Bani Widyatmiko, Maureen Koha, Ringga Saputra, Nur Izzahudinr, Sandya Ardi, Yeni Primasari, Ardya Dipta.</p><hr><p><strong>References</strong></p><p>[1] <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Devlin%2C+J\" rel=\"noopener\">J. Devlin</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Chang%2C+M\" rel=\"noopener\">M. Chang</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Lee%2C+K\" rel=\"noopener\">K. Lee</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Toutanova%2C+K\" rel=\"noopener\">K. Toutanova</a>: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.<br><a href=\"https://arxiv.org/abs/1810.04805\" rel=\"noopener\">arXiv:1810.04805</a> (2018)</p><p>[2] <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Lan%2C+Z\" rel=\"noopener\">Z. Lan</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Chen%2C+M\" rel=\"noopener\">M. Chen</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Goodman%2C+S\" rel=\"noopener\">S. Goodman</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Gimpel%2C+K\" rel=\"noopener\">K. Gimpel</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Sharma%2C+P\" rel=\"noopener\">P. Sharma</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Soricut%2C+R\" rel=\"noopener\">R. Soricut</a>: ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. <a href=\"https://arxiv.org/abs/1909.11942\" rel=\"noopener\">arXiv:1909.11942</a> (2019)</p><p>[3] <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Liu%2C+M\" rel=\"noopener\">M.Liu</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Tu%2C+Z\" rel=\"noopener\">Z. Tu</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+Z\" rel=\"noopener\">Z. Wang</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Xu%2C+X\" rel=\"noopener\">X. Xu</a>: LTP: A New Active Learning Strategy for Bert-CRF Based Named Entity Recognition. <a href=\"https://arxiv.org/abs/2001.02524\" rel=\"noopener\">arXiv:2001.02524</a> (2020)</p><hr><p>Liked what you read? <a href=\"https://mailchi.mp/go-jek/gojek-tech-newsletter\" rel=\"noopener\">Sign up for our newsletter</a> to have our latest stories delivered straight to your inbox!</p>","url":"https://gojek-ghost.zysk.in/how-gojek-uses-nlp-to-name-pickup-locations-at-scale-2/","canonical_url":null,"uuid":"51dd4492-b728-4b37-8c7f-41fd41619f7a","page":null,"codeinjection_foot":null,"codeinjection_head":null,"codeinjection_styles":null,"comment_id":"5eb0f8af5524cd001e7391f4","reading_time":6}},"pageContext":{"slug":"how-gojek-uses-nlp-to-name-pickup-locations-at-scale-2"}}}