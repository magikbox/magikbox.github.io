{"componentChunkName":"component---src-templates-post-js","path":"/blog/reducing-latency-with-sidecar/","result":{"data":{"ghostPost":{"id":"Ghost__Post__5ecb637d7aa22c4066f83f36","title":"Reducing Latency with Sidecar","slug":"reducing-latency-with-sidecar","featured":false,"feature_image":"https://gojek-ghost.zysk.in/content/images/2020/05/1_HWgBO6jgmlmk6-7_ewf7Yg.jpeg","excerpt":"How the sidecar pattern helped in our ongoing quest to reduce latency in our experimentation platform.","custom_excerpt":"How the sidecar pattern helped in our ongoing quest to reduce latency in our experimentation platform.","visibility":"public","created_at_pretty":"25 May, 2020","published_at_pretty":"18 November, 2019","updated_at_pretty":"25 May, 2020","created_at":"2020-05-25T11:49:41.000+05:30","published_at":"2019-11-18T09:30:00.000+05:30","updated_at":"2020-05-25T11:54:40.000+05:30","meta_title":"Reducing Latency with Sidecar","meta_description":"How the sidecar pattern helped in our ongoing quest to reduce latency in our experimentation platform.","og_description":null,"og_image":null,"og_title":null,"twitter_description":null,"twitter_image":null,"twitter_title":null,"authors":[{"name":"Gojek","slug":"gojek","bio":"Gojek is a Super App. It‚Äôs one app for ordering food, commuting, digital payments, shopping, hyper-local delivery, and two dozen services.","profile_image":"https://gojek-ghost.zysk.in/content/images/2020/05/logo-01-1.png","twitter":"@gojektech","facebook":"gojektech","website":"https://www.gojek.io"}],"primary_author":{"name":"Gojek","slug":"gojek","bio":"Gojek is a Super App. It‚Äôs one app for ordering food, commuting, digital payments, shopping, hyper-local delivery, and two dozen services.","profile_image":"https://gojek-ghost.zysk.in/content/images/2020/05/logo-01-1.png","twitter":"@gojektech","facebook":"gojektech","website":"https://www.gojek.io"},"primary_tag":{"name":"Tech","slug":"tech","description":"Learnings from technical challenges solved at Gojek, how-tos, and programming tips.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"},"tags":[{"name":"Tech","slug":"tech","description":"Learnings from technical challenges solved at Gojek, how-tos, and programming tips.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"}],"plaintext":"You‚Äôve probably heard about Litmus [http://bit.ly/2RGxvbg], Gojek‚Äôs in-house\nexperimentation platform. Since we built it, the adoption of Litmus across Gojek\nteams has been steadily increasing. More experiments are executed and more\nclients (in this case, Gojek teams) are integrated.\n\nBesides this, another requirement is to integrate Litmus with other backend\nservices directly, instead of solely with the mobile app. However, various\nclient backend services have their own SLA and most of them are linked to\nlatency. So, in order to client requirements, we have to reduce Litmus latency\nas much as possible.\n\nIn this post, we talk about our thought process behind delivering an initial\nsidecar design to achieve this goal,\n\nKnow your enemies\nFirst thing first, we needed to figure out where the bottleneck was. After\ndigging through some data, it turned out 30% of total time in a Litmus API call\nwas dominated by database call, and the number of database calls is ~2000 qps.\n\nBased on this fact, we decided to use cache. Now, we obviously could not use an\nexternal cache like Redis or Memcached. If we used those, we would be adding an\nadditional call. üòë\n\nInstead, we decided to use in-memory cache. Since Litmus was written in Clojure,\nwe went with caffeine cache [https://github.com/ben-manes/caffeine], which is\ngood enough to be used in JVM. We implemented the in-memory cache and the\ndatabase calls dropped from ~2000 qps to ~10 qps. The API latency also dropped\naround 10%.\n\nGood, but Not Good Enough. We Needed More.\nThe implementation of in-memory cache and further tinkering with Litmus\nprocesses would not be enough on their own. Our next approach was to reduce the\nhops between client and Litmus servers.\n\nHow? By moving Litmus server so it is geographically closer to the client?\n\n> Nah, we can do one better.\nWe pulled our Litmus server as close as possible to the backend service client.\nTo achieve this, we used sidecar pattern. We created a Litmus sidecar that\nresides in the same box as the backend service client. A Litmus sidecar can be\nseen as a mini version of the Litmus server that specifically serves the\ndedicated backend service client in the same box (VM).\n\nTweaking the Nuts and Bolts\nTo know how effective the sidecar pattern is, we need to know the base latency\nof Litmus server. We created 10 new active experiments specifically to get the\nbase latency number. With 20 concurrency and 10 request per second, the 99\npercentile was 23.55 ms.\n\nNeed faster API calls? Use gRPC\nWe used gRPC over REST wherever we could in our Litmus sidecar. gRPC also has\nvarious features such as push/pull streaming and client-side load balancing that\ncan be utilised for further enhancement. It‚Äôs fun stuff! üòÅ\n\nHow do we maintain our sidecar without permission to maintain the backend\nservice client box?\nIn Gojek, each team can only have permission to access its own components/boxes.\nIt is not possible (or scalable) to access all our backend service client boxes.\n\nWe could have considered this if we had only two or three clients. However, when\nyou‚Äôre building for an organisation of Gojek‚Äôs scale with the hope of having all\nteams adopt Litmus in their services, we couldn‚Äôt go down this route.\n\nOn the other hand, we still needed the ability to check the health of Litmus\nsidecars in every backend service client and develop a sidecar app that complies\nwith several 12 factor apps [https://12factor.net/] criteria (treat logs as\nevent streams and store config in the environment).\n\nSource\n[https://d33wubrfki0l68.cloudfront.net/24c41d54613afb774bff0a383043f8a47f4d0fd7/a14fb/static/img/consul-services.png]\nWe decided to use Consul for service discovery\n[https://www.consul.io/discovery.html] and key-value config\n[https://www.consul.io/docs/agent/kv.html]. In addition being easy to use and\nhaving a nice dashboard UI, Consul service discovery also supports health\nchecks.\n\nAfter we set up the infra (Consul server, agent, etc), we just needed to add our\nservice definition config along with our litmus sidecar to the backend service\nclient box. For the Consul KV itself, Consul supports hot reloading, which means\nwe don‚Äôt need to restart clients‚Äô litmus sidecar if there are config changes.\n\nHowever, we needed to handle the logic of hot reloading in the litmus sidecar\napp by ourself. For the logging part, Gojek already has a logging tools called \nBarito\n[https://blog.gojekengineering.com/how-we-built-barito-to-enhance-logging-19f80b89496f] \nthat forwards the system logs so that we don‚Äôt have to access client boxes with\nssh.\n\nHere is the high-level architecture of how litmus sidecar integrates with client\nbackend services. Instead of pointing to Litmus server address, now backend\nservice clients only need to point to its localhost with Litmus sidecar port.\n\nLitmus sidecar in the same box as client backend serviceHow is the data\ndistributed?\nAs an agent of Litmus server that resides on the same box as the client, Litmus\nsidecar needs to have access to the same data as Litmus server.\n\n> This was something we discussed long and hard. The trade off between speed and\ndata consistency, the trade off between consistency and availability, and so on.\nAs our purpose was to reduce latency, speed became our first class citizen. At\nleast for now, it is okay for us to have eventually consistent data in our\nLitmus sidecar. We also decided to persist the data in the client box. We don‚Äôt\nstore all Litmus server data, only what the client needs.\n\nWe wanted our Litmus sidecar as light as possible, so it does not disturb the\nmain process in the box. Instead of using some SQL database, we decided to use\nKV store BadgerDB for persistence storage.\n\nHow to synchronise data between Litmus sidecar and server?\nWe came up with several ideas around this, but decided to use the simplest one\nfor our initial design. Litmus sidecar pulls the data that it needs periodically\nand persists it with BadgerDB.\n\nAlrighty then, let‚Äôs measure how good our sidecar is\nUsing the same setup as we did in measuring base latency, the 99 percentile\nlatency dropped to 6.91 ms. ‚úå\n\nNew clients are now integrating with sidecar, but we didn‚Äôt stop there. We are\ncontinuously optimising our Litmus sidecar app, and you can read more about our\nefforts in this post [http://bit.ly/2LQb6a3].\n\nSo, that was the thought process behind building sidecar to reduce latency. We\nbelieve there is still room for improvement, and would love to hear your\nthoughts in the comments.\n\nWant our stories sent straight to your inbox? Sign up for our newsletter!\n[https://mailchi.mp/go-jek/gojek-tech-newsletter]\n\ngojek.jobs [http://bit.ly/2KslIe4]","html":"<p>You‚Äôve probably heard about <a href=\"http://bit.ly/2RGxvbg\" rel=\"noopener\">Litmus</a>, Gojek‚Äôs in-house experimentation platform. Since we built it, the adoption of Litmus across Gojek teams has been steadily increasing. More experiments are executed and more clients (in this case, Gojek teams) are integrated.</p><p>Besides this, another requirement is to integrate Litmus with other backend services directly, instead of solely with the mobile app. However, various client backend services have their own SLA and most of them are linked to latency. So, in order to client requirements, we have to reduce Litmus latency as much as possible.</p><p>In this post, we talk about our thought process behind delivering an initial sidecar design to achieve this goal,</p><h3 id=\"know-your-enemies\">Know your enemies</h3><p>First thing first, we needed to figure out where the bottleneck was. After digging through some data, it turned out 30% of total time in a Litmus API call was dominated by database call, and the number of database calls is ~2000 qps.</p><p>Based on this fact, we decided to use cache. Now, we obviously could not use an external cache like Redis or Memcached. If we used those, we would be adding an additional call. üòë</p><p>Instead, we decided to use in-memory cache. Since Litmus was written in Clojure, we went with <a href=\"https://github.com/ben-manes/caffeine\" rel=\"noopener\">caffeine cache</a>, which is good enough to be used in JVM. We implemented the in-memory cache and the database calls dropped from ~2000 qps to ~10 qps. The API latency also dropped around 10%.</p><h3 id=\"good-but-not-good-enough-we-needed-more-\">Good, but Not Good Enough. We Needed More.</h3><p>The implementation of in-memory cache and further tinkering with Litmus processes would not be enough on their own. Our next approach was to reduce the hops between client and Litmus servers.</p><p><strong><strong>How? By moving Litmus server so it is geographically closer to the client?</strong></strong></p><blockquote>Nah, we can do one better.</blockquote><p>We pulled our Litmus server as close as possible to the backend service client. To achieve this, we used <strong><strong>sidecar pattern</strong></strong>. We created a Litmus sidecar that resides in the same box as the backend service client. A Litmus sidecar can be seen as a mini version of the Litmus server that specifically serves the dedicated backend service client in the same box (VM).</p><h3 id=\"tweaking-the-nuts-and-bolts\">Tweaking the Nuts and Bolts</h3><p>To know how effective the sidecar pattern is, we need to know the base latency of Litmus server. We created 10 new active experiments specifically to get the base latency number. With 20 concurrency and 10 request per second, the 99 percentile was 23.55 ms.</p><h3 id=\"need-faster-api-calls-use-grpc\">Need faster API calls? Use gRPC</h3><p>We used gRPC over REST wherever we could in our Litmus sidecar. gRPC also has various features such as push/pull streaming and client-side load balancing that can be utilised for further enhancement. It‚Äôs fun stuff! üòÅ</p><h3 id=\"how-do-we-maintain-our-sidecar-without-permission-to-maintain-the-backend-service-client-box\">How do we maintain our sidecar without permission to maintain the backend service client box?</h3><p>In Gojek, each team can only have permission to access its own components/boxes. It is not possible (or scalable) to access all our backend service client boxes.</p><p>We could have considered this if we had only two or three clients. However, when you‚Äôre building for an organisation of Gojek‚Äôs scale with the hope of having all teams adopt Litmus in their services, we couldn‚Äôt go down this route.</p><p>On the other hand, we still needed the ability to check the health of Litmus sidecars in every backend service client and develop a sidecar app that complies with several <a href=\"https://12factor.net/\" rel=\"noopener\">12 factor apps</a> criteria (treat logs as event streams and store config in the environment).</p><figure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"><img src=\"https://gojek-ghost.zysk.in/content/images/2020/05/1_DpUpHDHGgk3d_jXEJYgD-w.png\" class=\"kg-image\"><figcaption><a href=\"https://d33wubrfki0l68.cloudfront.net/24c41d54613afb774bff0a383043f8a47f4d0fd7/a14fb/static/img/consul-services.png\">Source</a></figcaption></figure><p>We decided to use Consul for <a href=\"https://www.consul.io/discovery.html\" rel=\"noopener\">service discovery</a> and <a href=\"https://www.consul.io/docs/agent/kv.html\" rel=\"noopener\">key-value config</a>. In addition being easy to use and having a nice dashboard UI, Consul service discovery also supports health checks.</p><p>After we set up the infra (Consul server, agent, etc), we just needed to add our service definition config along with our litmus sidecar to the backend service client box. For the Consul KV itself, Consul supports hot reloading, which means we don‚Äôt need to restart clients‚Äô litmus sidecar if there are config changes.</p><p>However, we needed to handle the logic of hot reloading in the litmus sidecar app by ourself. For the logging part, Gojek already has a logging tools called <a href=\"https://blog.gojekengineering.com/how-we-built-barito-to-enhance-logging-19f80b89496f\" rel=\"noopener\">Barito</a> that forwards the system logs so that we don‚Äôt have to access client boxes with ssh.</p><p>Here is the high-level architecture of how litmus sidecar integrates with client backend services. Instead of pointing to Litmus server address, now backend service clients only need to point to its localhost with Litmus sidecar port.</p><figure class=\"kg-card kg-image-card kg-width-wide kg-card-hascaption\"><img src=\"https://gojek-ghost.zysk.in/content/images/2020/05/1_nlCPCXxdSLGuXKvrjjBdfA.png\" class=\"kg-image\"><figcaption>Litmus sidecar in the same box as client backend service</figcaption></figure><h3 id=\"how-is-the-data-distributed\">How is the data distributed?</h3><p>As an agent of Litmus server that resides on the same box as the client, Litmus sidecar needs to have access to the same data as Litmus server.</p><blockquote><em><em>This was something we discussed long and hard. The trade off between speed and data consistency, the trade off between consistency and availability, and so on.</em></em></blockquote><p>As our purpose was to reduce latency, speed became our first class citizen. At least for now, it is okay for us to have eventually consistent data in our Litmus sidecar. We also decided to persist the data in the client box. We don‚Äôt store all Litmus server data, only what the client needs.</p><p>We wanted our Litmus sidecar as light as possible, so it does not disturb the main process in the box. Instead of using some SQL database, we decided to use KV store BadgerDB for persistence storage.</p><h3 id=\"how-to-synchronise-data-between-litmus-sidecar-and-server\">How to synchronise data between Litmus sidecar and server?</h3><p>We came up with several ideas around this, but decided to use the simplest one for our initial design. Litmus sidecar pulls the data that it needs periodically and persists it with BadgerDB.</p><h3 id=\"alrighty-then-let-s-measure-how-good-our-sidecar-is\">Alrighty then, let‚Äôs measure how good our sidecar is</h3><p>Using the same setup as we did in measuring base latency, the 99 percentile latency dropped to 6.91 ms. ‚úå</p><p>New clients are now integrating with sidecar, but we didn‚Äôt stop there. We are continuously optimising our Litmus sidecar app, and you can read more about our efforts in this <a href=\"http://bit.ly/2LQb6a3\" rel=\"noopener\">post</a>.</p><p>So, that was the thought process behind building sidecar to reduce latency. We believe there is still room for improvement, and would love to hear your thoughts in the comments.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://gojek-ghost.zysk.in/content/images/2020/05/1_Yigf1nGxRKjzV7vIC_YkYg-1.png\" class=\"kg-image\"></figure><p>Want our stories sent straight to your inbox? <a href=\"https://mailchi.mp/go-jek/gojek-tech-newsletter\" rel=\"noopener\">Sign up for our newsletter!</a></p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://gojek-ghost.zysk.in/content/images/2020/05/1_XSAkmYAomyHp6_18rjjAfA-1.jpeg\" class=\"kg-image\"><figcaption><a href=\"http://bit.ly/2KslIe4\">gojek.jobs</a></figcaption></figure>","url":"https://gojek-ghost.zysk.in/reducing-latency-with-sidecar/","canonical_url":null,"uuid":"fad08e62-a341-4366-b20c-ae2c5c5f487d","page":null,"codeinjection_foot":null,"codeinjection_head":null,"codeinjection_styles":null,"comment_id":"5ecb637d7aa22c4066f83f36","reading_time":5},"tags":{"edges":[{"node":{"name":"Culture","slug":"culture"}},{"node":{"name":"Data","slug":"data"}},{"node":{"name":"Design","slug":"design"}},{"node":{"name":"News","slug":"news"}},{"node":{"name":"Stories","slug":"stories"}},{"node":{"name":"Tech","slug":"tech"}},{"node":{"name":"Maps","slug":"maps"}},{"node":{"name":"Ride Hailing","slug":"ride-hailing"}},{"node":{"name":"Software Engineering","slug":"software-engineering"}},{"node":{"name":"Startup","slug":"startup"}}]}},"pageContext":{"slug":"reducing-latency-with-sidecar"}}}