{"componentChunkName":"component---src-templates-post-js","path":"/blog/applying-the-single-responsibility-principle-to-microservices/","result":{"data":{"ghostPost":{"id":"Ghost__Post__5ec2af8a7aa22c4066f83abe","title":"Applying the Single Responsibility Principle to Microservices","slug":"applying-the-single-responsibility-principle-to-microservices","featured":false,"feature_image":"https://gojek-ghost.zysk.in/content/images/2020/05/1_5lEpe0Ab9138LC9cY1QZSQ.jpeg","excerpt":"The single responsibility principle helped us fix Icebreaker, our chat service. This is how we did it.","custom_excerpt":"The single responsibility principle helped us fix Icebreaker, our chat service. This is how we did it.","visibility":"public","created_at_pretty":"18 May, 2020","published_at_pretty":"05 February, 2019","updated_at_pretty":"18 May, 2020","created_at":"2020-05-18T21:23:46.000+05:30","published_at":"2019-02-05T09:30:00.000+05:30","updated_at":"2020-05-18T21:30:57.000+05:30","meta_title":null,"meta_description":"The single responsibility principle helped us fix Icebreaker, our chat service. This is how we did it.","og_description":null,"og_image":null,"og_title":null,"twitter_description":null,"twitter_image":null,"twitter_title":null,"authors":[{"name":"Gojek","slug":"gojek","bio":null,"profile_image":null,"twitter":"@gojektech","facebook":null,"website":"http://www.gojek.io"}],"primary_author":{"name":"Gojek","slug":"gojek","bio":null,"profile_image":null,"twitter":"@gojektech","facebook":null,"website":"http://www.gojek.io"},"primary_tag":{"name":"Tech","slug":"tech","description":"Learnings from technical challenges solved at Gojek, how-tos, and programming tips.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"},"tags":[{"name":"Tech","slug":"tech","description":"Learnings from technical challenges solved at Gojek, how-tos, and programming tips.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"}],"plaintext":"By Soham Kamani\n\nThe single responsibility principle is one of the most tried-and-tested tenets\nof software design. Every module or class should do one thing, and do it well.\nWe found this principle was applicable, and incredibly important, while\ndesigning our systems.\n\nAbout a year ago, we released a new chat service (which we internally called \nIcebreaker). It allowed our users to communicate with drivers through the app\nitself, rather than use SMS (which cost both the driver and customer money).\n\nHowever, all was not well. For numerous reasons, the service gave us a lot of\nissues and late night pagers. This post details the lessons we learnt, and some\nof the decisions we took to make this service more reliable.\n\nThe Problem(s)\nIn a nutshell, Icebreaker depended on too many other services to function\nproperly. Let‚Äôs look at some of the tasks Icebreaker performed in order to\ncreate a channel:\n\n 1. Authorise the API call: This made a call to our authentication service.\n 2. Fetch the customer profile: This required an HTTP call to our customer\n    service.\n 3. Fetch the drivers‚Äô profile: This required an HTTP call to our driver\n    service.\n 4. Verify if the customer-driver pair are in an active order: This made a call\n    to our active booking storage service.\n 5. Create the channel.\n\nIf any of these services failed, Icebreaker would fail as well.\n\nSimultaneous dependence on multiple services ensures that the dependent service\nis less stable than any of themEven if we could ensure 99% uptime for all\nservices in question, that still means the chances of all of them being up at\nthe same time was 96%.\n\nP(icebreaker active) = P(customer service active) * P(driver service active) *\nP(authentication service active) * P(active booking storage active) = 0.99 *\n0.99 * 0.99 * 0.99 ~= 0.96\n\nThis means our downtime has increased four times over (4%, as opposed to 1%).\n\nNot my job\nWhen a service starts to do too many things, it‚Äôs bound to fail sooner or later.\nIn this case, Icebreaker‚Äôs job was to create a channel between a customer and a\ndriver. However, it was doing all this extra stuff: like authentication,\nverification, and profile retrieval. ü§¶‚Äç‚ôÇ\n\nLet‚Äôs take a look at the changes we made to get rid of each dependency:\n\nAuthentication\nEvery API call arriving to Icebreaker came with an API token which needed\nauthentication. To solve this, we added a Kong [https://konghq.com/] API\ngateway. This authenticated all requests and added information about the\nauthenticated user within the API headers.\n\nNow, every request arriving to Icebreaker was authenticated.\n\n> Key takeaway: Tell, don‚Äôt ask. The requests coming from the API gateway told the\nservice that they were authenticated, rather than Icebreaker having to ask\nanother service.\nProfile retrieval\nIn order to create a channel, we needed a piece of information called the ‚Äòchat\ntoken‚Äô for each user. This was stored in the customer service for the customer,\nand the driver service for the driver.\n\nSince Icebreaker was the only service using this token, we moved these tokens to\nit, and removed them from the customer and driver services.\n\nNow,Icebreaker had all the information it needed in its own database, which was\na more reliable source of truth as compared to a whole other HTTP service.\n\n> Key takeaway: If your service is the only one using any piece of information, it\nshould reside within the service itself\nActive booking storage\nIcebreaker used to create a channel on-demand every time the user hit its\nchannel creation API. This on-demand creation required us to verify that an\nactive booking existed, for which the user needed to create a channel. After\nall, it didn‚Äôt make sense to create a channel when the parties involved did not\nhave an order with each other.\n\nTo fix this, we moved to an asynchronous architecture for channel creation.\nInstead of on-demand channel creation, we made use of GO-JEKs data pipeline\n[https://blog.gojekengineering.com/data-infrastructure-at-go-jek-cd4dc8cbd929],\nthat published events every time a booking was made. Icebreaker now consisted of\ntwo components: the worker and the server.\n\n 1. The worker consumed booking events every time they were made. It then\n    created a channel between the customer and driver in the booking, and stored\n    the channel information on a Redis cache.\n 2. The server served channel creation requests as before. Only, this time, the\n    channels were already created and cached, along with the order number.\n    \n\nSo, instead of on demand channel creation, the channels were created and stored\nbeforehand. Since we were consuming booking events from our own data pipeline,\nthere was no need to verify whether the booking was genuine or not.\n\n> Key takeaway: Again, tell, don‚Äôt ask. The events coming from our data pipeline\ntold Icebreaker that the bookings were genuine. This meant it could create the\nchannel, instead of hitting a service to verify the authenticity of the booking.\nResults\nNow Icebreaker did only what it was truly meant to do: create channels.\n\nSince we removed dependencies on most external systems, we no longer had to\nworry about one system failure causing Icebraker to malfunction. The load on the\nexternal services also reduced, since Icebreaker was no longer using their\nendpoints for channel creation.\n\nMoving to the asynchronous architecture also led to a drastic reduction in\nresponse time, from ~200ms to ~10ms, since we were pre-creating and caching\nchannels for every order.\n\nThe takeaways we got from this experience conform with the single responsibility\nprinciple. In the end, it‚Äôs always better to ask ourselves: ‚ÄúCan this service do\nless?‚Äù\n\n\n--------------------------------------------------------------------------------\n\nWant our stories in your inbox? Sign up for our newsletter!\n[https://mailchi.mp/go-jek/gojek-tech-newsletter]","html":"<p>By Soham Kamani</p><p>The single responsibility principle is one of the most tried-and-tested tenets of software design. Every module or class should do one thing, and do it well. We found this principle was applicable, and incredibly important, while designing our systems.</p><p>About a year ago, we released a new chat service (which we internally called <strong><strong>Icebreaker</strong></strong>). It allowed our users to communicate with drivers through the app itself, rather than use SMS (which cost both the driver and customer money).</p><p>However, all was not well. For numerous reasons, the service gave us a lot of issues and late night pagers. This post details the lessons we learnt, and some of the decisions we took to make this service more reliable.</p><h1 id=\"the-problem-s-\">The Problem(s)</h1><p>In a nutshell, Icebreaker depended on too many other services to function properly. Let‚Äôs look at some of the tasks Icebreaker performed in order to create a channel:</p><ol><li>Authorise the API call: This made a call to our authentication service.</li><li>Fetch the customer profile: This required an HTTP call to our customer service.</li><li>Fetch the drivers‚Äô profile: This required an HTTP call to our driver service.</li><li>Verify if the customer-driver pair are in an active order: This made a call to our active booking storage service.</li><li>Create the channel.</li></ol><figure class=\"kg-card kg-image-card\"><img src=\"https://miro.medium.com/max/1304/1*oSRgws5JNm767tOTJfq7EA.png\" class=\"kg-image\"></figure><p><strong><strong>If any of these services failed, Icebreaker would fail as well.</strong></strong></p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://miro.medium.com/max/1304/1*tpBpiBUBVK-N0chl5fkLGQ.png\" class=\"kg-image\"><figcaption>Simultaneous dependence on multiple services ensures that the dependent service is less stable than any of them</figcaption></figure><p>Even if we could ensure 99% uptime for all services in question, that still means the chances of all of them being up at the same time was 96%.</p><p><code>P(icebreaker active) = P(customer service active) * P(driver service active) * P(authentication service active) * P(active booking storage active) = 0.99 * 0.99 * 0.99 * 0.99 ~= 0.96</code></p><p>This means our downtime has increased four times over (4%, as opposed to 1%).</p><h1 id=\"not-my-job\">Not my job</h1><p>When a service starts to do too many things, it‚Äôs bound to fail sooner or later. In this case, Icebreaker‚Äôs job was to create a channel between a customer and a driver. However, it was doing all this extra stuff: like authentication, verification, and profile retrieval. ü§¶‚Äç‚ôÇ</p><p>Let‚Äôs take a look at the changes we made to get rid of each dependency:</p><h2 id=\"authentication\">Authentication</h2><p>Every API call arriving to Icebreaker came with an API token which needed authentication. To solve this, we added a <a href=\"https://konghq.com/\" rel=\"noopener\">Kong</a> API gateway. This authenticated all requests and added information about the authenticated user within the API headers.</p><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"https://miro.medium.com/max/1634/1*2d7v8u_WVx7YDowNapZGNQ.png\" class=\"kg-image\"></figure><p>Now, every request arriving to Icebreaker was authenticated.</p><blockquote><em><em><strong><strong><em>Key takeaway</em></strong></strong><em>: Tell, don‚Äôt ask. The requests coming from the API gateway told the service that they were authenticated, rather than Icebreaker having to ask another service.</em></em></em></blockquote><h2 id=\"profile-retrieval\">Profile retrieval</h2><p>In order to create a channel, we needed a piece of information called the ‚Äòchat token‚Äô for each user. This was stored in the customer service for the customer, and the driver service for the driver.</p><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"https://miro.medium.com/max/1634/1*dKBOVH4ODILeTSqgQIazcw.png\" class=\"kg-image\"></figure><p>Since Icebreaker was the only service using this token, we moved these tokens to it, and removed them from the customer and driver services.</p><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"https://miro.medium.com/max/1634/1*eXYrRFGNnEPqv0nvbwjbIA.png\" class=\"kg-image\"></figure><p>Now,Icebreaker had all the information it needed in its own database, which was a more reliable source of truth as compared to a whole other HTTP service.</p><blockquote><em><em><strong><strong><em>Key takeaway</em></strong></strong><em>: </em><em>If your service is the only one using any piece of information, it should reside within the service itself</em></em></em></blockquote><h1 id=\"active-booking-storage\">Active booking storage</h1><p>Icebreaker used to create a channel on-demand every time the user hit its channel creation API. This on-demand creation required us to verify that an active booking existed, for which the user needed to create a channel. After all, it didn‚Äôt make sense to create a channel when the parties involved did not have an order with each other.</p><p>To fix this, we moved to an asynchronous architecture for channel creation. Instead of on-demand channel creation, we made use of GO-JEKs <a href=\"https://blog.gojekengineering.com/data-infrastructure-at-go-jek-cd4dc8cbd929\" rel=\"noopener\">data pipeline</a>, that published events every time a booking was made. <strong><strong>Icebreaker now consisted of two components: the worker and the server.</strong></strong></p><ol><li>The worker consumed booking events every time they were made. It then created a channel between the customer and driver in the booking, and stored the channel information on a Redis cache.</li><li>The server served channel creation requests as before. Only, this time, the channels were already created and cached, along with the order number.<br></li></ol><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"https://miro.medium.com/max/3283/1*hNR_iSrTKkyvUhwMviHX_g.png\" class=\"kg-image\"></figure><p>So, instead of on demand channel creation, the channels were created and stored beforehand. Since we were consuming booking events from our own data pipeline, there was no need to verify whether the booking was genuine or not.</p><blockquote><em><em><strong><strong><em>Key takeaway</em></strong></strong><em>: Again, tell, don‚Äôt ask. The events coming from our data pipeline told Icebreaker that the bookings were genuine. This meant it could create the channel, instead of hitting a service to verify the authenticity of the booking.</em></em></em></blockquote><h1 id=\"results\">Results</h1><p>Now Icebreaker did only what it was truly meant to do: create channels.</p><p>Since we removed dependencies on most external systems, we no longer had to worry about one system failure causing Icebraker to malfunction. The load on the external services also reduced, since Icebreaker was no longer using their endpoints for channel creation.</p><p>Moving to the asynchronous architecture also led to a drastic reduction in response time, from ~200ms to ~10ms, since we were pre-creating and caching channels for every order.</p><p>The takeaways we got from this experience conform with the single responsibility principle. In the end, it‚Äôs always better to ask ourselves: <strong><strong><em><em>‚ÄúCan this service do less?‚Äù</em></em></strong></strong></p><hr><p>Want our stories in your inbox? <a href=\"https://mailchi.mp/go-jek/gojek-tech-newsletter\">Sign up for our newsletter!</a><br></p>","url":"https://gojek-ghost.zysk.in/applying-the-single-responsibility-principle-to-microservices/","canonical_url":null,"uuid":"8bb46a92-34cc-4bf4-a26e-65dc26a77873","page":null,"codeinjection_foot":null,"codeinjection_head":null,"codeinjection_styles":null,"comment_id":"5ec2af8a7aa22c4066f83abe","reading_time":4}},"pageContext":{"slug":"applying-the-single-responsibility-principle-to-microservices"}}}