{"componentChunkName":"component---src-templates-post-js","path":"/blog/beast-moving-data-from-kafka-to-bigquery-2/","result":{"data":{"ghostPost":{"id":"Ghost__Post__5eb0ff055524cd001e73924a","title":"Beast: Moving Data from Kafka to BigQuery","slug":"beast-moving-data-from-kafka-to-bigquery-2","featured":false,"feature_image":"https://res-2.cloudinary.com/hcq4cvthp/image/upload/q_auto/v1/ghost-blog-images/1_dc5zljnB3Xzp17C7sZ3d-Q.jpg","excerpt":"Gojek's open source solution for rapid movement of data from Kafka to Google BigQuery","custom_excerpt":"Gojek's open source solution for rapid movement of data from Kafka to Google BigQuery","visibility":"public","created_at_pretty":"05 May, 2020","published_at_pretty":"11 July, 2019","updated_at_pretty":"12 May, 2020","created_at":"2020-05-05T11:22:05.000+05:30","published_at":"2019-07-11T09:30:00.000+05:30","updated_at":"2020-05-12T11:53:53.000+05:30","meta_title":"Beast: Moving Data from Kafka to BigQuery","meta_description":"Gojek's open source solution for rapid movement of data from Kafka to Google BigQuery","og_description":null,"og_image":null,"og_title":null,"twitter_description":null,"twitter_image":null,"twitter_title":null,"authors":[{"name":"Gojek","slug":"gojek","bio":"Gojek is a Super App. It’s one app for ordering food, commuting, digital payments, shopping, hyper-local delivery, and two dozen services.","profile_image":"https://gojek-ghost.zysk.in/content/images/2020/05/logo-01-1.png","twitter":"@gojektech","facebook":"gojektech","website":"https://www.gojek.io"}],"primary_author":{"name":"Gojek","slug":"gojek","bio":"Gojek is a Super App. It’s one app for ordering food, commuting, digital payments, shopping, hyper-local delivery, and two dozen services.","profile_image":"https://gojek-ghost.zysk.in/content/images/2020/05/logo-01-1.png","twitter":"@gojektech","facebook":"gojektech","website":"https://www.gojek.io"},"primary_tag":{"name":"Data","slug":"data","description":"Updates on Gojek's work in Data Science and Data Engineering—from infrastructure development to our experiments with AI and ML.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"},"tags":[{"name":"Data","slug":"data","description":"Updates on Gojek's work in Data Science and Data Engineering—from infrastructure development to our experiments with AI and ML.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"}],"plaintext":"by Rajat Goyal [http://medium.com/@rajat404] and Maulik Soneji\n[https://medium.com/@maulik.soneji]\n\nIn order to serve customers across 19+ products, GOJEK places a lot of emphasis\non data. Our Data Warehouse [https://en.wikipedia.org/wiki/Data_warehouse],\nbuilt by integrating data from multiple applications and sources, helps our team\nof data scientists, as well as business and product analysts make solid,\ndata-driven decisions.\n\nThis post explains our open source solution for easy movement of data from Kafka\nto BigQuery.\n\nData Warehouse setup at GOJEK\nWe use Google Bigquery [https://cloud.google.com/bigquery/] (BQ) as our Data\nWarehouse, which serves as a powerful tool for interactive analysis. This has\nproven extremely valuable for our use cases.\n\nOur approach to push data to our warehouse is to first push the data to Kafka.\nWe rely on multiple Kafka clusters to ingest relevant events across teams.\n\nA common approach to push data from Kafka to BigQuery is to first push it to\nGCS, and then import said data into BigQuery from GCS. While this solves the use\ncase of running analytics on historical data, we also use BigQuery for\nnear-real-time analytics & reporting. This analysis in-turn provides valuable\ninsights to make the right business decisions in a short time frame.\n\nThe initial approach\nOur original implementation used an individual code base for each topic in Kafka\nand use them to push data to BQ.\n\nThis required a lot of maintenance in order to keep up with the new topics and\nnew fields to existing topics being added to Kafka. Such changes required the\nmanual intervention of a dev/analyst to update the schema in both code and BQ\ntable. We also witnessed incidents of data loss on a few occasions, which\nrequired manually loading data from GCS.\n\nThe need for a new solution\nNew topics are added almost every other day to the several Kafka clusters in the\norganisation. Given that GOJEK has expanded its operations to several countries,\nmanaging the agglomeration of individual scripts for each topic was a massive\nordeal.\n\nIn order to deal with scaling issues, we decided to write a new system from\nscratch and take into account learnings from our previous experiences. Our\nsolution was a system that could ingest all the data pushed to Kafka and write\nit to Bigquery.\n\nWe decided to call it ‘Beast’ as it has to ingest all data that is generated in\nGOJEK.\n\nBefore starting with development, we had the following requirements to take care\nof:\n\n * No data loss: Every single message should be pushed from Kafka to BQ at least\n   once\n * Single codebase: A single repository able to handle any proto schema, for any\n   topic, without any code changes\n * Scalability: The app needs to be able to handle substantially high throughput\n * Observability: A dev should be able to see the state of the system at any\n   given point of time\n * Painless upgrades: Updating the schema for a topic should be a simple\n   operation\n\nArchitecture\n\nBeast takes inspiration from Sakaar\n[https://blog.gojekengineering.com/sakaar-taking-kafka-data-to-cloud-storage-at-go-jek-7839da20b5f3]\n, our in-house solution for pushing data from Kafka to GCS. Like Sakaar, Beast\nutilises Java’s blocking queues, for consuming, processing, pushing and\ncommitting the messages. Blocking queues allow us to make each of these stages\nindependent of the other, letting us optimise each stage in and of itself.\n\nEach Beast instance packs the following components:\n\n * Consumer: A native Kafka consumer, which consumes messages in batches from\n   Kafka, translates them to BQ compatible format, and pushes all of them into\n   two blocking queues — Read Queue and Commit Queue.\n * BQ Workers: A group of worker threads which pick messages from the read\n   queue, and push to BigQuery. Once a message batch is pushed successfully, the\n   BQ worker adds the committed offset of the batch to the Acknowledgement Set.\n   This offset acts as an acknowledgement of the batch being successfully\n   pushed.\n * Kafka Committer: A thread which keeps polling the head of the commit queue,\n   to get the earliest message batch. The committer looks for the commit offset\n   of that batch in the Acknowledgement Set. If the acknowledgement is available\n   (implying that the batch was successfully pushed to BQ), then the offset of\n   that batch is committed back to Kafka, and it’s removed from the commit\n   queue.\n\nSalient Features\nBeast is entirely cloud native, thus scaling it is a piece of cake.\n\nFor high throughput topics, all we need to do is spawn more pods. Since Beast\nrelies on Kafka consumers, we can have as many consumers as the number of\npartitions, and as long as they have the same consumer group, Kafka will ensure\nthat all the consumers receive unique messages.\n\nBeast takes a proto-descriptor file, which defines the details of all the protos\nin the registry. It then simply picks the details of the proto, specified in the\nconfiguration. This allows us to use the same codebase for all deployments, and\nalso makes the upgrades a breeze.\n\nBeast is open source ? ?\nBeast is now part of the open source domain. Do give it a shot!\n\nYou can find Beast here: ?\n\nhttps://github.com/gojek/beast\n\nHelm chart for the same can be found here\n[https://github.com/gojektech/charts/tree/master/incubator/beast].\n\nContributions, criticism, feedback, and bug reports are always welcome. ?\n\n\n--------------------------------------------------------------------------------\n\nIf you like what you read and want our stories delivered straight to your inbox, \nsign up for our newsletter [https://mailchi.mp/go-jek/gojek-tech-newsletter].","html":"<p>by <a href=\"http://medium.com/@rajat404\" rel=\"noopener\">Rajat Goyal</a> and <a href=\"https://medium.com/@maulik.soneji\" rel=\"noopener\">Maulik Soneji</a></p><p>In order to serve customers across 19+ products, GOJEK places a lot of emphasis on data. Our <a href=\"https://en.wikipedia.org/wiki/Data_warehouse\" rel=\"noopener\">Data Warehouse</a>, built by integrating data from multiple applications and sources, helps our team of data scientists, as well as business and product analysts make solid, data-driven decisions.</p><p>This post explains our open source solution for easy movement of data from Kafka to BigQuery.</p><h1 id=\"data-warehouse-setup-at-gojek\">Data Warehouse setup at GOJEK</h1><p>We use <a href=\"https://cloud.google.com/bigquery/\" rel=\"noopener\">Google Bigquery</a> (BQ) as our Data Warehouse, which serves as a powerful tool for interactive analysis. This has proven extremely valuable for our use cases.</p><p><em><em>Our approach to push data to our warehouse is to first push the data to Kafka. We rely on multiple Kafka clusters to ingest relevant events across teams.</em></em></p><p>A common approach to push data from Kafka to BigQuery is to first push it to GCS, and then import said data into BigQuery from GCS. While this solves the use case of running analytics on historical data, we also use BigQuery for near-real-time analytics &amp; reporting. This analysis in-turn provides valuable insights to make the right business decisions in a short time frame.</p><h1 id=\"the-initial-approach\">The initial approach</h1><p>Our original implementation used an individual code base for each topic in Kafka and use them to push data to BQ.</p><p>This required a lot of maintenance in order to keep up with the new topics and new fields to existing topics being added to Kafka. Such changes required the manual intervention of a dev/analyst to update the schema in both code and BQ table. We also witnessed incidents of data loss on a few occasions, which required manually loading data from GCS.</p><h1 id=\"the-need-for-a-new-solution\">The need for a new solution</h1><p>New topics are added almost every other day to the several Kafka clusters in the organisation. Given that GOJEK has expanded its operations to several countries, managing the agglomeration of individual scripts for each topic was a massive ordeal.</p><p>In order to deal with scaling issues, we decided to write a new system from scratch and take into account learnings from our previous experiences. Our solution was a system that could ingest all the data pushed to Kafka and write it to Bigquery.</p><p>We decided to call it ‘<strong>Beast’</strong> as it has to ingest all data that is generated in GOJEK.</p><p>Before starting with development, we had the following requirements to take care of:</p><ul><li><strong><strong>No data loss: </strong></strong>Every single message should be pushed from Kafka to BQ at least once</li><li><strong><strong>Single codebase: </strong></strong>A single repository able to handle any proto schema, for any topic, without any code changes</li><li><strong><strong>Scalability:</strong></strong> The app needs to be able to handle substantially high throughput</li><li><strong><strong>Observability:</strong></strong> A dev should be able to see the state of the system at any given point of time</li><li><strong><strong>Painless upgrades:</strong></strong> Updating the schema for a topic should be a simple operation</li></ul><h1 id=\"architecture\">Architecture<br></h1><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"https://miro.medium.com/max/1373/1*MlBXLLD33_MwOyILGivskg.png\" class=\"kg-image\"></figure><p>Beast takes inspiration from <a href=\"https://blog.gojekengineering.com/sakaar-taking-kafka-data-to-cloud-storage-at-go-jek-7839da20b5f3\" rel=\"noopener\">Sakaar</a>, our in-house solution for pushing data from Kafka to GCS. Like Sakaar, Beast utilises Java’s blocking queues, for consuming, processing, pushing and committing the messages. Blocking queues allow us to make each of these stages independent of the other, letting us optimise each stage in and of itself.</p><p>Each Beast instance packs the following components:</p><ul><li><strong><strong>Consumer:</strong></strong> A native Kafka consumer, which consumes messages in batches from Kafka, translates them to BQ compatible format, and pushes all of them into two blocking queues — Read Queue and Commit Queue.</li><li><strong><strong>BQ Workers: </strong></strong>A group of worker threads which pick messages from the read queue, and push to BigQuery. Once a message batch is pushed successfully, the BQ worker adds the committed offset of the batch to the Acknowledgement Set. This offset acts as an acknowledgement of the batch being successfully pushed.</li><li><strong><strong>Kafka Committer:</strong></strong> A thread which keeps polling the head of the commit queue, to get the earliest message batch. The committer looks for the commit offset of that batch in the Acknowledgement Set. If the acknowledgement is available (implying that the batch was successfully pushed to BQ), then the offset of that batch is committed back to Kafka, and it’s removed from the commit queue.</li></ul><h1 id=\"salient-features\">Salient Features</h1><p>Beast is entirely cloud native, thus scaling it is a piece of cake.</p><p>For high throughput topics, all we need to do is spawn more pods. Since Beast relies on Kafka consumers, we can have as many consumers as the number of partitions, and as long as they have the same consumer group, Kafka will ensure that all the consumers receive unique messages.</p><p>Beast takes a proto-descriptor file, which defines the details of all the protos in the registry. It then simply picks the details of the proto, specified in the configuration. This allows us to use the same codebase for all deployments, and also makes the upgrades a breeze.</p><h1 id=\"beast-is-open-source-\">Beast is open source ? ?</h1><p>Beast is now part of the open source domain. Do give it a shot!</p><p><strong><strong>You can find Beast here: ?</strong></strong></p><p><a href=\"https://github.com/gojek/beast\">https://github.com/gojek/beast</a></p><p>Helm chart for the same can be found <a href=\"https://github.com/gojektech/charts/tree/master/incubator/beast\" rel=\"noopener\">here</a>.</p><p>Contributions, criticism, feedback, and bug reports are always welcome. ?</p><hr><p>If you like what you read and want our stories delivered straight to your inbox, <a href=\"https://mailchi.mp/go-jek/gojek-tech-newsletter\">sign up for our newsletter</a>. </p>","url":"https://gojek-ghost.zysk.in/beast-moving-data-from-kafka-to-bigquery-2/","canonical_url":null,"uuid":"3c91fa54-e0f7-43a2-88d8-55bf7fd56c9c","page":null,"codeinjection_foot":null,"codeinjection_head":null,"codeinjection_styles":null,"comment_id":"5eb0ff055524cd001e73924a","reading_time":4},"tags":{"edges":[{"node":{"name":"Culture","slug":"culture"}},{"node":{"name":"Data","slug":"data"}},{"node":{"name":"Design","slug":"design"}},{"node":{"name":"News","slug":"news"}},{"node":{"name":"Stories","slug":"stories"}},{"node":{"name":"Tech","slug":"tech"}},{"node":{"name":"Maps","slug":"maps"}},{"node":{"name":"Ride Hailing","slug":"ride-hailing"}},{"node":{"name":"Software Engineering","slug":"software-engineering"}},{"node":{"name":"Startup","slug":"startup"}}]}},"pageContext":{"slug":"beast-moving-data-from-kafka-to-bigquery-2"}}}